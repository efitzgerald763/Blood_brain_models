{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRXTS3jqcFhcCnDf9FzJc8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efitzgerald763/Blood_brain_models/blob/main/ENSG00000096060_pediction_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connects to your Google Drive so you can import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/Blood_brain_pred/ENSG00000096060_blood_brain.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Set the index to the first column\n",
        "data.set_index(data.columns[0], inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9VHNOaD1bji",
        "outputId": "266dedb5-de2f-493c-d983-c9b17501807f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose the dataframe so each gene is a feature and each sample is a column\n",
        "data_transposed = data.T\n",
        "\n",
        "data_transposed.sample(4)\n",
        "\n",
        "# Separate the target variable\n",
        "target_row = 'ENSG00000096060'\n",
        "y = data_transposed[target_row]\n",
        "X = data_transposed.drop(columns=[target_row])\n",
        "\n",
        "# Check the shapes to ensure they are as expected\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJrSPP_o1eh0",
        "outputId": "ef82211a-52ec-43a3-8108-19d771398def"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(81, 18706)\n",
            "(81,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(repr(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4c7Wue65dka",
        "outputId": "27d71b8c-38b9-47d3-d723-0895389554a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array([[ 3.03405882, -0.96819402,  2.17954197, ...,  2.36694902,\n",
            "        -0.77987753,  6.15849995],\n",
            "       [ 3.8070011 , -0.61224533,  3.64087908, ...,  1.84159704,\n",
            "         0.2790302 ,  7.32233393],\n",
            "       [ 3.79537299, -2.17211446,  2.65980449, ...,  2.56276484,\n",
            "        -1.6279202 ,  6.67240443],\n",
            "       ...,\n",
            "       [ 2.73369283, -0.97929489,  1.86685639, ...,  2.61390675,\n",
            "         0.03446319,  6.11528062],\n",
            "       [ 3.37911512, -1.49087285,  2.94839323, ...,  2.23670283,\n",
            "         1.01114867,  6.63725929],\n",
            "       [ 3.35029368, -0.97792172,  3.64203891, ...,  2.24725401,\n",
            "        -0.46298198,  6.67478364]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.any(np.isnan(X_scaled))  # Should be False\n",
        "np.any(np.isinf(X_scaled))  # Should be False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F6zuO6m8PMg",
        "outputId": "8351c27b-904d-4bd1-9f79-5bccd066187f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define the VAE model\n",
        "original_dim = X_scaled.shape[1]\n",
        "intermediate_dim = 64\n",
        "latent_dim = 10\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(original_dim,))\n",
        "h = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "# Decoder\n",
        "decoder_h = Dense(intermediate_dim, activation='relu')\n",
        "decoder_mean = Dense(original_dim, activation='linear')\n",
        "h_decoded = decoder_h(z)\n",
        "x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "# VAE model\n",
        "vae = Model(inputs, x_decoded_mean)\n",
        "\n",
        "# Loss function\n",
        "xent_loss = original_dim * tf.keras.losses.mean_squared_error(inputs, x_decoded_mean)\n",
        "kl_loss = -0.5 * K.mean(z_log_var - K.square(z_mean) - K.exp(z_log_var) + 1, axis=-1)\n",
        "vae_loss = K.mean(xent_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "\n",
        "# Train the VAE\n",
        "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
        "vae.fit(X_train, epochs=100, batch_size=10, validation_data=(X_test, None))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0ALaaSe7ETi",
        "outputId": "b0faa36e-bf20-4417-cc2e-a9b4e803c403"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - 2s 98ms/step - loss: 17740.4785 - val_loss: 19914.9766\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 55ms/step - loss: 18302.6855 - val_loss: 19927.8008\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 18604.2129 - val_loss: 19903.3359\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 18735.0254 - val_loss: 19801.8555\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 17798.3477 - val_loss: 19842.7109\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 17494.0664 - val_loss: 19457.8828\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 17724.9785 - val_loss: 19153.9180\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 17028.6289 - val_loss: 18883.3828\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 16477.8555 - val_loss: 18612.9941\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 15480.5918 - val_loss: 18207.5273\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 14837.9346 - val_loss: 17847.3164\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 14790.3682 - val_loss: 17577.3906\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 14336.9326 - val_loss: 17375.4453\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 13922.9629 - val_loss: 17058.2031\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 13813.9434 - val_loss: 16995.8086\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 13360.8105 - val_loss: 17046.6172\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 13167.7793 - val_loss: 16647.8203\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 12851.1162 - val_loss: 16642.0156\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 12881.8330 - val_loss: 16517.5605\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 12972.3975 - val_loss: 16542.6445\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 12424.0674 - val_loss: 16290.8584\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 12240.0186 - val_loss: 16536.7598\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 12231.1016 - val_loss: 16208.1484\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 12324.8994 - val_loss: 16218.4961\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 11787.3701 - val_loss: 16463.1309\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 12200.8906 - val_loss: 16098.4092\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 11424.6855 - val_loss: 16023.8945\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 11438.0283 - val_loss: 16428.6035\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 11187.9912 - val_loss: 15999.4473\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 11055.0928 - val_loss: 15964.8066\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 11085.3223 - val_loss: 15635.5098\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 10924.0020 - val_loss: 15666.9053\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 10680.7793 - val_loss: 15709.1992\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 10620.6045 - val_loss: 15598.5332\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 10354.8340 - val_loss: 15879.3906\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 10656.8135 - val_loss: 15580.8301\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 10342.8408 - val_loss: 16112.0381\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 10108.4697 - val_loss: 15549.7695\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 10048.5498 - val_loss: 15603.2129\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 10051.1377 - val_loss: 16919.1074\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 9889.3799 - val_loss: 15651.9863\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 9870.4121 - val_loss: 15834.5820\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 9568.6104 - val_loss: 17131.5859\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 9778.3369 - val_loss: 16368.5723\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 9581.9561 - val_loss: 16171.0547\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 9380.0225 - val_loss: 15408.6992\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 9462.4219 - val_loss: 24397.6367\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 9227.2148 - val_loss: 15435.0098\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 9280.9746 - val_loss: 17053.0879\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 9027.4512 - val_loss: 15511.3750\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 9001.1582 - val_loss: 15241.7461\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 0s 60ms/step - loss: 8883.3037 - val_loss: 15321.4795\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 9004.6572 - val_loss: 15082.9688\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 8714.4072 - val_loss: 15473.2598\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 8751.6318 - val_loss: 16172.4082\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 8697.6084 - val_loss: 15338.2578\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 8557.0410 - val_loss: 16024.6914\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 8514.5771 - val_loss: 15056.0176\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 8396.4805 - val_loss: 16916.7148\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 8479.6689 - val_loss: 16147.4062\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 8371.6094 - val_loss: 15022.1074\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 8284.1377 - val_loss: 16735.5332\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 8231.0449 - val_loss: 15810.5400\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 8150.9282 - val_loss: 15508.1191\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 8018.5664 - val_loss: 23006.8711\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 8048.7661 - val_loss: 15811.0381\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 8076.9858 - val_loss: 14988.0254\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 7872.7368 - val_loss: 19895.9336\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 8098.4722 - val_loss: 15907.5977\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 7901.4697 - val_loss: 15844.5586\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 7843.4717 - val_loss: 15906.5137\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 7730.1753 - val_loss: 14902.0547\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 7798.1646 - val_loss: 16182.9258\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 7722.3940 - val_loss: 15259.7363\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 7560.1597 - val_loss: 19902.6309\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 7514.6523 - val_loss: 16052.4629\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 7473.5400 - val_loss: 15753.7686\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 7472.6797 - val_loss: 17874.5820\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 7308.5747 - val_loss: 18563.1680\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 7264.2422 - val_loss: 15804.5020\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 7295.4517 - val_loss: 16254.9355\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 7231.6226 - val_loss: 14903.3203\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 7227.6787 - val_loss: 15278.7451\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 7143.2588 - val_loss: 15850.0293\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 7218.9253 - val_loss: 14838.5791\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 7119.6216 - val_loss: 16589.8281\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 7237.6235 - val_loss: 14813.2480\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 6941.7983 - val_loss: 25049.9375\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 7006.0171 - val_loss: 14961.7246\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 7072.2612 - val_loss: 14840.7256\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 6964.5454 - val_loss: 15560.6592\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 6889.0342 - val_loss: 16112.9023\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 6874.7427 - val_loss: 16079.9727\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 6784.8931 - val_loss: 15608.0703\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 6697.6562 - val_loss: 21270.9414\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 6795.7534 - val_loss: 15371.0176\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 6645.1616 - val_loss: 15178.9922\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 60ms/step - loss: 6803.5503 - val_loss: 24840.2715\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 6630.5972 - val_loss: 15478.1016\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 6542.6577 - val_loss: 14976.2109\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7adfed42b5e0>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the encoder model to get the latent representation\n",
        "encoder = Model(inputs, z_mean)\n",
        "X_latent = encoder.predict(X)\n",
        "print(X_latent.shape)  # Should be (number_of_samples, latent_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLB9aAxe4key",
        "outputId": "63e26c88-cad1-480d-8af5-17434a535b61"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7adffead8af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 7ms/step\n",
            "(81, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_train_latent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcR4lu4u4wpn",
        "outputId": "c2a9e98d-beea-46d2-9c9b-62b11763dc02"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 15.689438    26.845959     0.59926075 -11.96184    -14.809234\n",
            "  -33.61141     18.695398   -15.169431   -12.615536   -31.20485   ]\n",
            " [ 42.754055    67.13723     25.131374    -4.68932    -10.655405\n",
            "  -47.533516    27.133953   -44.12864    -14.287205   -59.317932  ]\n",
            " [ 39.4025      66.87071     21.2883     -34.418865   -16.041552\n",
            "  -15.779143    -8.038779   -35.867805   -55.539803   -73.835144  ]\n",
            " [ 39.9587      47.415714     1.4067799  -35.20488    -10.944722\n",
            "  -48.83126     17.814318   -40.89168     -5.1457534  -73.02159   ]\n",
            " [ 26.637852    50.549175    10.245051   -19.034674   -19.604063\n",
            "  -34.391716    25.789162   -22.725407   -33.908966   -56.21167   ]\n",
            " [ 17.85808     45.440144    -6.2488923  -22.02026    -26.94512\n",
            "  -36.604046    26.845697   -25.014387   -24.173822   -46.49655   ]\n",
            " [ 17.897196    53.100338    28.669956    -7.3657684  -17.072565\n",
            "  -32.16342     25.526258   -18.436577   -24.42217    -46.20988   ]\n",
            " [ 26.489698    47.92831     32.49885    -29.59282      7.2570662\n",
            "  -31.212673    31.369202   -25.051651   -58.484543   -58.32251   ]\n",
            " [ 19.947031    62.668526    55.588615     4.2220182  -10.533206\n",
            "  -34.91023     14.553853   -23.984642   -45.03563    -56.94283   ]\n",
            " [ 28.625786    44.899685    43.4328     -21.586964    -3.2556465\n",
            "  -33.804813    11.16413     -8.697652   -54.25778    -50.811028  ]\n",
            " [ 23.154385    54.179302    -6.87078     -3.2790067  -17.381018\n",
            "  -26.209435    21.43045    -26.331469   -13.135437   -32.7641    ]\n",
            " [ 33.699074    65.67393     34.16656      7.392456    -1.2550431\n",
            "  -37.486088    16.65698    -20.674547   -27.772041   -60.11901   ]\n",
            " [ 26.384026    62.61455     18.06224     -5.5596375   -7.925327\n",
            "  -28.882502    18.623892   -23.093346   -27.16811    -50.107693  ]\n",
            " [ 48.352886    72.46        27.548536    -9.785332     2.9382575\n",
            "  -38.585827    23.254208   -39.91437     -1.1959925  -64.59517   ]\n",
            " [ 39.133224    58.59857      3.8276575   -2.6785553   -5.7946634\n",
            "  -42.115223    30.877544   -26.948473    -9.46863    -50.524323  ]\n",
            " [ 22.831116    56.0174      31.843525   -29.87553    -27.801647\n",
            "   -3.3105478   -6.1099324    8.473042   -77.43774    -42.470085  ]\n",
            " [ 13.425766    53.294216     1.6282201  -10.929862   -23.358059\n",
            "  -19.585892    18.220957   -17.390322   -44.516575   -52.149757  ]\n",
            " [ 16.16006     57.903877     9.485765    -9.387055   -13.501476\n",
            "  -21.190388    13.832334   -23.284904   -34.599476   -48.610302  ]\n",
            " [ 12.974072    54.70192     24.921188    -8.150894   -20.765684\n",
            "  -18.312        9.199694   -22.415867   -51.307045   -63.45017   ]\n",
            " [ 21.513592    68.557175     5.2534566   -1.6640382   -8.231752\n",
            "  -22.20449     29.473475   -13.619347   -34.70966    -46.527775  ]\n",
            " [ 29.2289      40.12029     28.267988   -36.63979    -17.319115\n",
            "  -16.950157    11.027735   -21.482582   -54.81689    -79.52844   ]\n",
            " [ 36.98583     50.59645     26.670631   -20.39244      0.64450663\n",
            "  -28.364695    17.334284   -20.605028   -40.120678   -64.716774  ]\n",
            " [ 16.920738    46.23074     23.45286     -9.206579   -20.746397\n",
            "  -23.77215     13.018712   -13.167241   -46.07974    -44.88369   ]\n",
            " [ 22.556149    65.946365    17.124197    -6.6743536   -8.948076\n",
            "  -23.768475    17.941504   -23.352983   -39.508488   -53.860374  ]\n",
            " [  9.213934    29.990618     7.4288983  -23.34693    -19.679966\n",
            "  -21.233692    11.131828   -16.745487   -45.226555   -37.261417  ]\n",
            " [ 34.657494    49.88547     36.513893    -6.113639    -7.029831\n",
            "  -35.483246    37.041904   -15.714885   -52.323254   -64.18316   ]\n",
            " [ 17.000698    63.777786     2.1071625   -9.959692   -23.68415\n",
            "  -18.382597    15.578477   -16.91643    -37.497856   -50.292507  ]\n",
            " [ 28.803524    60.96212     41.915676     5.0148067   -4.9750967\n",
            "  -24.345419    12.594413    -7.68015    -43.630047   -52.096214  ]\n",
            " [ 29.543915    45.683872    12.247934    -2.3484638   11.711493\n",
            "  -35.111637    21.677288   -13.543621   -19.267017   -33.995064  ]\n",
            " [ 21.885256    36.797344    -4.9849257  -37.624756   -37.400097\n",
            "    3.6297495   -5.836042   -12.312662   -61.05479    -34.376106  ]\n",
            " [ 20.524233    57.394012    -5.604806   -18.215073   -27.589087\n",
            "  -20.618494    14.25267    -28.544586   -39.63407    -51.926174  ]\n",
            " [ 24.182507    71.328735    30.167286   -16.439566   -22.726992\n",
            "  -23.712849    19.286156   -28.817501   -60.52531    -72.10231   ]\n",
            " [ 28.814083    72.35403    -19.26103    -12.631753   -27.214415\n",
            "  -28.653955    20.885242   -37.90274    -16.256289   -51.30706   ]\n",
            " [ 16.559431    53.636856    -2.7946754  -11.004691   -19.964622\n",
            "  -19.224373    18.190907   -15.1816025  -29.672306   -40.27095   ]\n",
            " [ 10.885614    35.82475     28.45153    -10.352516    -9.034647\n",
            "    1.0079772   28.158932    -8.109207   -71.46709    -65.98127   ]\n",
            " [ 24.470945    47.421745    -3.6618435  -11.740947   -12.458077\n",
            "  -25.007414    10.184088   -21.450203   -20.587215   -41.856956  ]\n",
            " [ 32.37202     56.32076    -16.146704   -15.82588    -21.790882\n",
            "  -32.263298    17.568539   -39.574043    -9.450978   -38.63775   ]\n",
            " [ 29.405106    75.05381     42.518482     2.2884293  -13.83805\n",
            "  -14.488188    -2.8475912  -11.632976   -49.093452   -65.15687   ]\n",
            " [ 27.741098    70.6069      22.519264    -2.98114     -1.4769042\n",
            "  -28.600983    27.963247   -22.874834   -34.574562   -62.29556   ]\n",
            " [ 29.33477     46.677864     5.130206   -33.15124    -21.322863\n",
            "  -22.995539    -9.441719     1.227862   -28.813568   -44.512627  ]\n",
            " [ 29.34433     66.9395       4.0202565  -10.379196   -13.388683\n",
            "  -30.75961     22.784203   -31.255873   -25.75425    -48.966476  ]\n",
            " [  6.6651106   54.669304    -2.9676101   -4.997398   -32.3133\n",
            "  -26.93619     28.998777   -19.120256   -41.080055   -36.47624   ]\n",
            " [  8.531396    63.214035    32.00078    -14.340779   -24.91087\n",
            "  -30.817104     5.1909595  -29.217852   -58.07425    -55.532173  ]\n",
            " [ 10.935004    53.961853   -16.759867    -0.9444545  -26.260244\n",
            "  -27.785286    16.361382   -32.15602    -39.07039    -46.436497  ]\n",
            " [ 30.633455    56.80601     -0.39361855 -42.649284   -28.99034\n",
            "  -25.572987    13.106718   -26.728388   -43.4345     -60.96077   ]\n",
            " [  8.26412     45.611202    23.36244     -4.6656194  -19.947273\n",
            "  -22.289932    17.233036   -17.19789    -46.794468   -58.140358  ]\n",
            " [  5.491012    42.9953      -1.3954474  -14.110845   -36.005173\n",
            "  -22.107166    25.833782   -24.731674   -47.525284   -60.771633  ]\n",
            " [  8.255975    52.261936    -4.9664783  -20.083418   -31.403788\n",
            "  -22.30537     28.77923    -15.009087   -31.468845   -43.588123  ]\n",
            " [ 35.095367    65.003716     3.8789272   -0.09918866 -12.114167\n",
            "  -38.78591     24.77676    -47.16039    -13.785582   -57.933647  ]\n",
            " [ 24.03704     45.553368     6.4343796  -31.802473   -20.982744\n",
            "  -12.038923     2.1679287  -13.292875   -54.492893   -52.17485   ]\n",
            " [ 41.102688    45.515244     9.379265    -3.3798368   -5.814486\n",
            "  -41.385666    20.728954   -30.07366    -26.11486    -33.14275   ]\n",
            " [ 27.970497    60.90694     31.64057    -13.945637   -16.719559\n",
            "  -30.583208    13.1962185  -18.286268   -50.30931    -74.764824  ]\n",
            " [ 17.408209    73.940186     4.775765     3.256635   -23.543818\n",
            "  -28.797676    27.00177    -22.654114   -38.68786    -39.931683  ]\n",
            " [ 18.662468    70.22133     27.382114     5.8644876  -22.089415\n",
            "  -12.89428     13.836199   -26.223907   -64.388      -78.10077   ]\n",
            " [ 29.553717    55.019424    26.518099     3.0030015   -8.374143\n",
            "  -29.72045      8.868184   -15.238404   -25.05288    -50.74284   ]\n",
            " [ 36.334705    67.87152     22.628662    -7.4907947   -2.9629598\n",
            "  -31.640173    18.107895   -23.633467   -24.065403   -47.769325  ]\n",
            " [ 23.107962    24.815184   -16.768824   -28.338158   -17.823776\n",
            "  -32.740917    10.9094925  -16.721872   -37.031075   -33.421413  ]\n",
            " [  8.319771    34.980118    14.0079365    4.1947336  -34.166187\n",
            "  -40.508114    17.710331   -34.532166   -13.223794   -70.32179   ]\n",
            " [ 36.661552    65.997215   -15.902416   -27.72656    -22.49662\n",
            "   -0.8005179   22.418905    -6.7334595  -36.451103   -18.995277  ]\n",
            " [ 43.36928     72.58566     32.795246    -6.2446804  -10.170065\n",
            "  -42.77812     33.134624   -34.50743    -26.842585   -72.311516  ]\n",
            " [ 46.686344    59.02829     21.805662    -2.593941    -4.488029\n",
            "  -42.655518    26.345747   -30.762278   -34.263115   -63.223217  ]\n",
            " [ 33.983902    51.272266    11.488752   -28.363167   -20.777655\n",
            "  -31.380543     9.469889   -31.756351   -25.016975   -58.98008   ]\n",
            " [ 37.507168    54.671993    21.407238    -9.801502    -3.6248782\n",
            "  -40.132965    24.395885   -19.822859   -15.5546     -47.873775  ]\n",
            " [ 25.298223    48.11028     12.994335   -17.072823   -37.917053\n",
            "  -16.38984     15.486784   -34.248764   -50.522392   -54.219097  ]\n",
            " [ 30.396868    62.636147    15.807685   -10.392724    -7.569435\n",
            "  -31.244589    24.140911   -26.461166   -22.325958   -59.564247  ]\n",
            " [ 35.817135    67.74381      1.5712564  -31.840128   -28.702526\n",
            "  -33.541656    15.436295   -35.757374   -40.86723    -68.973305  ]\n",
            " [ 20.112892    36.864693     8.181656   -29.291492   -25.583763\n",
            "  -35.711105    28.723572   -27.262499   -50.04205    -69.11428   ]\n",
            " [ 34.806545    62.194954    16.295946    -9.251236   -11.802001\n",
            "  -42.597324    26.955853   -40.414997   -18.901356   -62.644894  ]\n",
            " [ 12.490284    60.445145    27.844728    -8.951797   -21.17269\n",
            "  -22.32586     19.14296    -18.522562   -54.257404   -58.840454  ]\n",
            " [ 31.289194    62.77661     23.474781   -33.12545     -8.025597\n",
            "  -25.171978    32.44306    -30.31787    -69.42146    -83.25442   ]\n",
            " [ 30.80803     57.748028     6.675664    -8.63213     -7.028106\n",
            "  -36.749893    22.853895   -33.010063   -13.003351   -51.393215  ]\n",
            " [ 42.782776    43.67514     23.125322   -20.133894    -0.7460833\n",
            "  -39.022213    15.40952    -25.206005   -19.558933   -62.26545   ]\n",
            " [ 23.482323    37.556152    24.3417     -15.250235   -21.961658\n",
            "  -32.786522    25.478796   -27.221502   -55.30087    -65.83723   ]\n",
            " [ 33.442375    38.80099     29.509348   -18.636976     2.8054922\n",
            "  -35.635597    21.655119   -16.06223    -24.395731   -58.245605  ]\n",
            " [ 42.34124     58.144417    12.565432    -6.0826206  -14.148806\n",
            "  -38.500103    25.01509    -23.500748   -12.180968   -41.91928   ]\n",
            " [ 35.5461      71.34073     54.631027   -12.850731    11.066159\n",
            "  -39.49568     19.717712   -17.039171   -56.299496   -71.74264   ]\n",
            " [ 37.333256    52.35526     19.643286    -8.597622    -4.467843\n",
            "  -35.351852    19.680374   -25.010822   -14.867333   -52.175823  ]\n",
            " [ 34.334854    61.126564    21.687298   -21.826376   -10.5310545\n",
            "  -29.286434    25.288378   -19.441883   -42.35232    -56.77519   ]\n",
            " [ 14.415256    41.996014    -7.985601   -13.39746    -26.190916\n",
            "  -28.522177    23.722328   -24.460352   -16.510729   -31.077993  ]\n",
            " [ 36.72182     57.29805      6.0973086  -14.737466    -9.9086685\n",
            "  -38.55398     27.772902   -25.35699     -9.872434   -51.96246   ]\n",
            " [ 42.840763    60.24931     24.218678     1.8148898   -5.8534904\n",
            "  -48.600708    21.953823   -30.923607   -23.501135   -45.915764  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Split the data\n",
        "X_train_latent, X_test_latent, y_train, y_test = train_test_split(X_latent, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the linear regression model\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train_latent, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = regressor.predict(X_test_latent)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(f'R^2 Score: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6nmUNdZ4oCQ",
        "outputId": "ea6c0d4d-7163-4867-bdca-8b0ac3481044"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.49867569602344064\n",
            "R^2 Score: 0.07913427291915687\n"
          ]
        }
      ]
    }
  ]
}